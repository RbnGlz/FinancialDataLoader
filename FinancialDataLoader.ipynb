{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f06e5d-74f7-49d9-9ec0-87b4925a0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  FinancialDataLoader  ###\n",
    "\"\"\"\n",
    "Contiene la clase DataLoader que implementa:\n",
    "    Descarga de datos financieros con caché.\n",
    "    Procesamiento en paralelo.\n",
    "    Funciones de visualización de precios.\n",
    "\"\"\"   \n",
    "# Importar librerías\n",
    "import pandas_datareader as pdr\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Tuple, Union\n",
    "import yfinance as yf\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import time\n",
    "import hashlib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuración de logging para ver mensajes informativos en consola\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Decorador para medir el tiempo de ejecución de funciones\n",
    "def timer_decorator(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        logger.debug(f\"Función {func.__name__} ejecutada en {end_time - start_time:.4f} segundos\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Clase DataLoader\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Clase para cargar y procesar datos financieros de múltiples fuentes.\n",
    "    Características:\n",
    "    - Caché de datos para evitar descargas repetidas\n",
    "    - Procesamiento en paralelo para mejorar la eficiencia\n",
    "    - Visualización de datos financieros\n",
    "    \"\"\"\n",
    "    # Fuentes de datos soportadas\n",
    "    SUPPORTED_SOURCES = {\"yahoo\", \"stooq\"}\n",
    "    \n",
    "    def __init__(self, fecha_inicio: str = \"2020-01-01\", \n",
    "                 fecha_final: str = None,\n",
    "                 cache_dir: str = \"data_cache\",\n",
    "                 use_cache: bool = True,\n",
    "                 cache_expiry_days: int = 1):\n",
    "        \"\"\"\n",
    "        Inicializa el cargador de datos financieros.\n",
    "        Args:\n",
    "            fecha_inicio: Fecha de inicio para los datos (formato \"YYYY-MM-DD\").\n",
    "            fecha_final: Fecha final para los datos (formato \"YYYY-MM-DD\"). \n",
    "                         Si es None, se usa la fecha actual.\n",
    "            cache_dir: Directorio para almacenar la caché de datos.\n",
    "            use_cache: Si es True, se utilizará la caché para evitar descargas repetidas.\n",
    "            cache_expiry_days: Número de días tras los cuales la caché se considera expirada.\n",
    "        \"\"\"\n",
    "        # Configurar fecha final como la fecha actual si no se proporciona\n",
    "        if fecha_final is None:\n",
    "            fecha_final = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        # Guardamos ambas representaciones: string y datetime\n",
    "        self.fecha_inicio_str = fecha_inicio\n",
    "        self.fecha_final_str = fecha_final\n",
    "        self.fecha_inicio = pd.to_datetime(fecha_inicio)\n",
    "        self.fecha_final = pd.to_datetime(fecha_final)\n",
    "        # Validar fechas\n",
    "        if self.fecha_inicio > self.fecha_final:\n",
    "            raise ValueError(\"La fecha de inicio debe ser anterior a la fecha final\")\n",
    "        # Configuración de caché\n",
    "        self.use_cache = use_cache\n",
    "        self.cache_expiry_days = cache_expiry_days\n",
    "        # Convertimos el directorio de caché a objeto Path y lo creamos\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        if self.use_cache:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # Diccionario para almacenar datos en memoria\n",
    "        self._memory_cache = {}\n",
    "    \n",
    "    def _get_cache_filename(self, ticker: str, source: str) -> str:\n",
    "        \"\"\"\n",
    "        Genera un nombre de archivo único para la caché basado en los parámetros.\n",
    "        Usa un hash para manejar caracteres especiales en los tickers.\n",
    "        \"\"\"\n",
    "        # Crear un hash para evitar problemas con caracteres especiales en nombres de archivo\n",
    "        params_str = f\"{ticker}_{source}_{self.fecha_inicio_str}_{self.fecha_final_str}\"\n",
    "        filename_hash = hashlib.md5(params_str.encode()).hexdigest()\n",
    "        return f\"{ticker}_{source}_{filename_hash}.pkl\"\n",
    "    \n",
    "    def _get_cache_path(self, ticker: str, source: str) -> Path:\n",
    "        \"\"\"\n",
    "        Genera la ruta del archivo de caché para un ticker y fuente específicos.\n",
    "        \"\"\"\n",
    "        filename = self._get_cache_filename(ticker, source)\n",
    "        return self.cache_dir / filename\n",
    "    \n",
    "    def _is_cache_valid(self, cache_path: Path) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica si un archivo de caché es válido basado en su fecha de modificación.\n",
    "        Returns:\n",
    "            True si la caché es válida, False si ha expirado o no existe.\n",
    "        \"\"\"\n",
    "        if not cache_path.exists():\n",
    "            return False\n",
    "        # Verificar si la caché ha expirado\n",
    "        if self.cache_expiry_days > 0:\n",
    "            mod_time = datetime.fromtimestamp(cache_path.stat().st_mtime)\n",
    "            expiry_time = datetime.now() - timedelta(days=self.cache_expiry_days)\n",
    "            if mod_time < expiry_time:\n",
    "                logger.debug(f\"Caché expirada para {cache_path.name}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _load_from_cache(self, ticker: str, source: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Intenta cargar datos desde la caché.\n",
    "        Primero verifica la caché en memoria, luego la caché en disco.\n",
    "        Returns:\n",
    "            DataFrame con los datos si la caché es válida, None en caso contrario.\n",
    "        \"\"\"\n",
    "        if not self.use_cache:\n",
    "            return None\n",
    "        # Verificar caché en memoria\n",
    "        memory_key = f\"{ticker}_{source}\"\n",
    "        if memory_key in self._memory_cache:\n",
    "            logger.debug(f\"Datos cargados desde caché en memoria para {ticker} de {source}\")\n",
    "            return self._memory_cache[memory_key]\n",
    "        # Verificar caché en disco\n",
    "        cache_path = self._get_cache_path(ticker, source)\n",
    "        if self._is_cache_valid(cache_path):\n",
    "            try:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                # Almacenar en caché de memoria para acceso más rápido\n",
    "                self._memory_cache[memory_key] = data\n",
    "                logger.info(f\"Datos cargados desde caché en disco para {ticker} de {source}\")\n",
    "                return data\n",
    "            except (pickle.PickleError, EOFError, AttributeError) as e:\n",
    "                logger.warning(f\"Error al cargar caché para {ticker}: {e}\")\n",
    "                # Eliminar archivo de caché corrupto\n",
    "                cache_path.unlink(missing_ok=True)\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, ticker: str, source: str, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Guarda los datos en caché (memoria y disco).\n",
    "        \"\"\"\n",
    "        if not self.use_cache or data is None or data.empty:\n",
    "            return\n",
    "        # Guardar en caché de memoria\n",
    "        memory_key = f\"{ticker}_{source}\"\n",
    "        self._memory_cache[memory_key] = data\n",
    "        # Guardar en caché de disco\n",
    "        cache_path = self._get_cache_path(ticker, source)\n",
    "        try:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            logger.debug(f\"Datos guardados en caché para {ticker} de {source}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error al guardar caché para {ticker}: {e}\")\n",
    "    \n",
    "    @timer_decorator\n",
    "    def _get_data_from_source(self, ticker: str, source: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Obtiene datos de una fuente específica, utilizando caché si está disponible.\n",
    "        Args:\n",
    "            ticker: Símbolo del activo financiero.\n",
    "            source: Fuente de datos (\"yahoo\" o \"stooq\").\n",
    "        Returns:\n",
    "            DataFrame con los datos financieros o None si hay error.\n",
    "        \"\"\"\n",
    "        # Validar fuente\n",
    "        if source not in self.SUPPORTED_SOURCES:\n",
    "            logger.error(f\"Fuente {source} no soportada. Fuentes disponibles: {self.SUPPORTED_SOURCES}\")\n",
    "            return None\n",
    "        # Intentar cargar datos desde la caché\n",
    "        cached_data = self._load_from_cache(ticker, source)\n",
    "        if cached_data is not None:\n",
    "            return cached_data\n",
    "        try:\n",
    "            if source == \"stooq\":\n",
    "                df = pdr.get_data_stooq(symbols=ticker, \n",
    "                                        start=self.fecha_inicio, \n",
    "                                        end=self.fecha_final)\n",
    "            elif source == \"yahoo\":\n",
    "                df = yf.download(ticker, \n",
    "                                 start=self.fecha_inicio, \n",
    "                                 end=self.fecha_final,\n",
    "                                 progress=False)\n",
    "            else:\n",
    "                return None  # Ya validamos antes, pero por si acaso\n",
    "            # Verificar si se obtuvieron datos\n",
    "            if df is None or df.empty:\n",
    "                logger.warning(f\"No se encontraron datos para {ticker} en {source}\")\n",
    "                return None\n",
    "            # Ordenar el DataFrame cronológicamente usando el índice (fechas)\n",
    "            if not isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "            df = df.sort_index(ascending=True)\n",
    "            # Guardar en caché para evitar descargas futuras innecesarias\n",
    "            self._save_to_cache(ticker, source, df)\n",
    "            return df\n",
    "        except ConnectionError as ce:\n",
    "            logger.error(f\"Error de conexión al descargar datos para {ticker}: {ce}\")\n",
    "            raise ConnectionError(f\"No se pudo conectar al servicio de datos: {ce}\")\n",
    "        except ValueError as ve:\n",
    "            logger.error(f\"Error de valor al descargar datos para {ticker}: {ve}\")\n",
    "            raise\n",
    "        except Exception as error:\n",
    "            logger.error(f\"Error inesperado al obtener datos de {source} para {ticker}: {error}\")\n",
    "            return None\n",
    "    \n",
    "    def _process_ticker(self, args: Tuple[str, str, bool]) -> Optional[Tuple[str, pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Función auxiliar para el procesamiento en paralelo.\n",
    "        Args:\n",
    "            args: Una tupla que contiene (ticker, fuente, show_progress).\n",
    "        Returns:\n",
    "            Una tupla (clave, DataFrame) si la descarga es exitosa, o None en caso de error.\n",
    "        \"\"\"\n",
    "        ticker, source, _ = args\n",
    "        df = self._get_data_from_source(ticker, source)\n",
    "        if df is not None:\n",
    "            return (f\"{ticker}_{source}\", df)\n",
    "        return None\n",
    "    \n",
    "    @timer_decorator\n",
    "    def load_parallel(self, tickers: List[str], \n",
    "                      sources: List[str] = None,\n",
    "                      executor_type: str = \"thread\",\n",
    "                      show_progress: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carga datos en paralelo usando un pool de hilos o procesos.\n",
    "        Args:\n",
    "            tickers: Lista de símbolos (tickers) a descargar.\n",
    "            sources: Lista de fuentes desde las cuales se descargan los datos.\n",
    "                    Si es None, se usan todas las fuentes soportadas.\n",
    "            executor_type: \"thread\" para ThreadPoolExecutor o \"process\" para ProcessPoolExecutor.\n",
    "            show_progress: Si es True, muestra una barra de progreso.\n",
    "        Returns:\n",
    "            Un diccionario donde la clave es el ticker concatenado con la fuente y el valor es el DataFrame.\n",
    "        \"\"\"\n",
    "        # Usar todas las fuentes soportadas si no se especifica\n",
    "        if sources is None:\n",
    "            sources = list(self.SUPPORTED_SOURCES)\n",
    "        else:\n",
    "            # Filtrar fuentes no soportadas\n",
    "            sources = [s for s in sources if s in self.SUPPORTED_SOURCES]\n",
    "            if not sources:\n",
    "                logger.error(f\"Ninguna fuente especificada es soportada. Fuentes disponibles: {self.SUPPORTED_SOURCES}\")\n",
    "                return {}\n",
    "        results = {}\n",
    "        # Se genera la lista de tareas para cada combinación de ticker y fuente\n",
    "        tasks = [(ticker, source, show_progress) for ticker in tickers for source in sources]\n",
    "        # Determinar el número óptimo de trabajadores\n",
    "        num_cpus = multiprocessing.cpu_count()\n",
    "        num_workers = min(num_cpus, len(tasks))\n",
    "        # Seleccionar el tipo de ejecutor según el argumento\n",
    "        if executor_type.lower() == \"process\":\n",
    "            Executor = ProcessPoolExecutor\n",
    "            logger.info(f\"Usando ProcessPoolExecutor con {num_workers} workers\")\n",
    "        else:\n",
    "            Executor = ThreadPoolExecutor\n",
    "            logger.info(f\"Usando ThreadPoolExecutor con {num_workers} workers\")\n",
    "        # Ejecutar las tareas en paralelo\n",
    "        with Executor(max_workers=num_workers) as executor:\n",
    "            # Usar tqdm para mostrar progreso si se solicita\n",
    "            if show_progress:\n",
    "                futures = list(tqdm(executor.map(self._process_ticker, tasks), \n",
    "                                   total=len(tasks), \n",
    "                                   desc=\"Descargando datos\"))\n",
    "            else:\n",
    "                futures = list(executor.map(self._process_ticker, tasks))\n",
    "            for result in futures:\n",
    "                if result:\n",
    "                    key, df = result\n",
    "                    results[key] = df\n",
    "        logger.info(f\"Datos cargados exitosamente para {len(results)}/{len(tasks)} combinaciones\")\n",
    "        return results\n",
    "    \n",
    "    def load_single(self, ticker: str, source: str = \"yahoo\") -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carga datos para un solo ticker y fuente.\n",
    "        Es un método de conveniencia para cuando solo se necesita un activo.\n",
    "        Args:\n",
    "            ticker: Símbolo del activo financiero.\n",
    "            source: Fuente de datos (\"yahoo\" o \"stooq\").\n",
    "        Returns:\n",
    "            DataFrame con los datos financieros o None si hay error.\n",
    "        \"\"\"\n",
    "        if source not in self.SUPPORTED_SOURCES:\n",
    "            logger.error(f\"Fuente {source} no soportada. Fuentes disponibles: {self.SUPPORTED_SOURCES}\")\n",
    "            return None\n",
    "        return self._get_data_from_source(ticker, source)\n",
    "    \n",
    "    def plot_prices(self, data: Dict[str, pd.DataFrame], \n",
    "                    column: str = \"Close\",\n",
    "                    ma_periods: List[int] = None,\n",
    "                    figsize: Tuple[int, int] = (22, 12),\n",
    "                    normalize: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Grafica los precios de múltiples activos con opciones avanzadas.\n",
    "        Args:\n",
    "            data: Diccionario de DataFrames con datos financieros.\n",
    "            column: Columna que se desea graficar (por defecto \"Close\").\n",
    "            ma_periods: Lista de períodos para calcular medias móviles.\n",
    "            figsize: Tamaño de la figura (ancho, alto).\n",
    "            normalize: Si es True, normaliza los precios para facilitar la comparación.\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            logger.warning(\"No hay datos para graficar\")\n",
    "            return\n",
    "        # Se establece un estilo moderno para el gráfico\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize=figsize)\n",
    "        # Preparar los datos para graficar\n",
    "        plot_data = {}\n",
    "        for key, df in data.items():\n",
    "            if column in df.columns:\n",
    "                series = df[column].copy()\n",
    "                # Normalizar los datos si se solicita\n",
    "                if normalize:\n",
    "                    series = series / series.iloc[0] * 100\n",
    "                plot_data[key] = series\n",
    "            else:\n",
    "                logger.warning(f\"La columna '{column}' no se encontró en los datos de {key}\")\n",
    "        # Graficar los datos\n",
    "        for key, series in plot_data.items():\n",
    "            series.plot(label=key, linewidth=2, alpha=0.8)\n",
    "            # Añadir medias móviles si se solicitan\n",
    "            if ma_periods:\n",
    "                for period in ma_periods:\n",
    "                    ma = series.rolling(window=period).mean()\n",
    "                    ma.plot(label=f\"{key} MA{period}\", linestyle='--', alpha=0.6)\n",
    "        # Configurar el gráfico\n",
    "        title = f\"Precios de {column}\"\n",
    "        if normalize:\n",
    "            title += \" (Normalizados, Base 100)\"\n",
    "            plt.ylabel(\"Precio Normalizado (Base 100)\", size=20)\n",
    "        else:\n",
    "            plt.ylabel(\"Precio\", size=20)\n",
    "        plt.title(title, size=25, pad=20)\n",
    "        plt.xlabel(\"Fecha\", size=20)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def clear_cache(self, tickers: List[str] = None, sources: List[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Limpia la caché para los tickers y fuentes especificados.\n",
    "        Si no se especifican tickers o fuentes, se limpia toda la caché.\n",
    "        Args:\n",
    "            tickers: Lista de tickers para los que se limpiará la caché.\n",
    "            sources: Lista de fuentes para las que se limpiará la caché.\n",
    "        \"\"\"\n",
    "        # Limpiar caché en memoria\n",
    "        if tickers is None and sources is None:\n",
    "            self._memory_cache.clear()\n",
    "            logger.info(\"Caché en memoria limpiada completamente\")\n",
    "        else:\n",
    "            # Limpiar selectivamente\n",
    "            if tickers is not None and sources is not None:\n",
    "                keys_to_remove = [f\"{ticker}_{source}\" for ticker in tickers for source in sources]\n",
    "            elif tickers is not None:\n",
    "                keys_to_remove = [k for k in self._memory_cache.keys() if any(k.startswith(f\"{ticker}_\") for ticker in tickers)]\n",
    "            else:  # sources is not None\n",
    "                keys_to_remove = [k for k in self._memory_cache.keys() if any(f\"_{source}\" in k for source in sources)]\n",
    "            for key in keys_to_remove:\n",
    "                if key in self._memory_cache:\n",
    "                    del self._memory_cache[key]\n",
    "            logger.info(f\"Se eliminaron {len(keys_to_remove)} entradas de la caché en memoria\")\n",
    "        # Limpiar caché en disco\n",
    "        if not self.use_cache or not self.cache_dir.exists():\n",
    "            return\n",
    "        if tickers is None and sources is None:\n",
    "            # Eliminar todos los archivos de caché\n",
    "            for cache_file in self.cache_dir.glob(\"*.pkl\"):\n",
    "                cache_file.unlink()\n",
    "            logger.info(\"Caché en disco limpiada completamente\")\n",
    "        else:\n",
    "            # Eliminar archivos selectivamente\n",
    "            count = 0\n",
    "            for cache_file in self.cache_dir.glob(\"*.pkl\"):\n",
    "                filename = cache_file.name\n",
    "                should_delete = False\n",
    "                if tickers is not None and sources is not None:\n",
    "                    should_delete = any(f\"{ticker}_{source}_\" in filename for ticker in tickers for source in sources)\n",
    "                elif tickers is not None:\n",
    "                    should_delete = any(f\"{ticker}_\" in filename for ticker in tickers)\n",
    "                else:  # sources is not None\n",
    "                    should_delete = any(f\"_{source}_\" in filename for source in sources)\n",
    "                if should_delete:\n",
    "                    cache_file.unlink()\n",
    "                    count += 1\n",
    "            logger.info(f\"Se eliminaron {count} archivos de caché en disco\")\n",
    "\n",
    "def setup_notebook():\n",
    "    \"\"\"\n",
    "    Configura el entorno del notebook para mostrar gráficos inline.\n",
    "    Esto es útil cuando se ejecuta en Jupyter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import IPython\n",
    "        IPython.get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        plt.style.use('seaborn')\n",
    "        logger.info(\"Entorno de notebook configurado correctamente\")\n",
    "    except (ImportError, AttributeError):\n",
    "        logger.debug(\"No se está ejecutando en un entorno de notebook\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Ejemplo de uso del DataLoader.\n",
    "    Se configura el entorno (en caso de estar en un notebook), se descargan datos de varios\n",
    "    tickers en paralelo y se grafican los precios de cierre.\n",
    "    \"\"\"\n",
    "    # Configurar el entorno del notebook\n",
    "    setup_notebook()\n",
    "    # Inicializar el cargador de datos con fecha final automática (hoy)\n",
    "    loader = DataLoader(fecha_inicio=\"2020-01-01\", fecha_final=None, cache_expiry_days=1)\n",
    "    # Definir una lista de tickers a procesar\n",
    "    tickers = [\"AMZN\", \"AAPL\", \"MSFT\", \"GOOGL\", \"META\"]\n",
    "    # Cargar datos en paralelo utilizando ThreadPoolExecutor (por defecto para operaciones IO-bound)\n",
    "    logger.info(\"Iniciando carga de datos...\")\n",
    "    data = loader.load_parallel(tickers, executor_type=\"thread\", show_progress=True)\n",
    "    logger.info(f\"Datos cargados exitosamente para {len(data)} combinaciones de ticker y fuente\")\n",
    "    # Graficar los precios de cierre\n",
    "    loader.plot_prices(data, ma_periods=[20, 50])\n",
    "    # Graficar los precios normalizados para comparación\n",
    "    loader.plot_prices(data, normalize=True)\n",
    "    # Ejemplo de limpieza de caché\n",
    "    # loader.clear_cache(tickers=[\"AAPL\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
