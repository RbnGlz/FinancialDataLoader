{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c4005-1bc3-4755-9c5b-7b622256cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FinancialDataLoader##\n",
    "\n",
    "\"\"\"\n",
    "Contiene la clase DataLoader que implementa:\n",
    "\n",
    "    Descarga de datos financieros con caché.\n",
    "    Procesamiento en paralelo.\n",
    "    Funciones de visualización de precios.\n",
    "\"\"\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c8b26-ec16-43d6-a2c4-1fd9a54e624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "\n",
    "import pandas_datareader as pdr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor  # Se importan ambos para escoger según necesidad\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "import yfinance as yf\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822773a6-f34d-4d73-b8cc-d02f0adbbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de logging para ver mensajes informativos en consola\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea7d54-7d56-4de0-9a70-182d01018c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clase DataLoader\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, fecha_inicio: str = \"2020-01-01\", \n",
    "                 fecha_final: str = \"2025-01-01\",\n",
    "                 cache_dir: str = \"data_cache\"):\n",
    "        \"\"\"\n",
    "        Inicializa el cargador de datos financieros.\n",
    "        \n",
    "        Se convierten las fechas de entrada a objetos datetime para mayor robustez\n",
    "        y se mantiene la versión en string para usarla en nombres de archivos de caché.\n",
    "        \n",
    "        Args:\n",
    "            fecha_inicio: Fecha de inicio para los datos (formato \"YYYY-MM-DD\").\n",
    "            fecha_final: Fecha final para los datos (formato \"YYYY-MM-DD\").\n",
    "            cache_dir: Directorio para almacenar la caché de datos.\n",
    "        \"\"\"\n",
    "        # Guardamos ambas representaciones: string y datetime\n",
    "        self.fecha_inicio_str = fecha_inicio\n",
    "        self.fecha_final_str = fecha_final\n",
    "        self.fecha_inicio = pd.to_datetime(fecha_inicio)\n",
    "        self.fecha_final = pd.to_datetime(fecha_final)\n",
    "        \n",
    "        # Convertimos el directorio de caché a objeto Path y lo creamos (incluyendo subdirectorios si es necesario)\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def _get_cache_path(self, ticker: str, source: str) -> Path:\n",
    "        \"\"\"\n",
    "        Genera la ruta del archivo de caché para un ticker y fuente específicos.\n",
    "        \n",
    "        Se usa la representación en string de las fechas para la consistencia del nombre.\n",
    "        \"\"\"\n",
    "        return self.cache_dir / f\"{ticker}_{source}_{self.fecha_inicio_str}_{self.fecha_final_str}.pkl\"\n",
    "    \n",
    "    def _load_from_cache(self, ticker: str, source: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Intenta cargar datos desde la caché.\n",
    "        \n",
    "        Se recomienda cargar solo archivos de fuentes de confianza, ya que el uso de pickle\n",
    "        puede ser riesgoso si se cargan datos no verificados.\n",
    "        \"\"\"\n",
    "        cache_path = self._get_cache_path(ticker, source)\n",
    "        if cache_path.exists():\n",
    "            try:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                logger.info(f\"Datos cargados desde caché para {ticker} de {source}\")\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                # Se recomienda capturar excepciones específicas en producción para mayor control\n",
    "                logger.warning(f\"Error al cargar caché para {ticker}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, ticker: str, source: str, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Guarda los datos en caché usando pickle.\n",
    "        \"\"\"\n",
    "        cache_path = self._get_cache_path(ticker, source)\n",
    "        try:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            logger.info(f\"Datos guardados en caché para {ticker} de {source}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error al guardar caché para {ticker}: {e}\")\n",
    "\n",
    "    def _get_data_from_source(self, ticker: str, source: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Obtiene datos de una fuente específica, utilizando caché si está disponible.\n",
    "        \n",
    "        Se utilizan dos fuentes:\n",
    "          - \"stooq\" a través de pandas_datareader.\n",
    "          - \"yahoo\" a través de yfinance.\n",
    "        \n",
    "        Las fechas se pasan como objetos datetime.\n",
    "        \"\"\"\n",
    "        # Intentar cargar datos desde la caché\n",
    "        cached_data = self._load_from_cache(ticker, source)\n",
    "        if cached_data is not None:\n",
    "            return cached_data\n",
    "        \n",
    "        try:\n",
    "            if source == \"stooq\":\n",
    "                # pdr.get_data_stooq utiliza los parámetros de fecha\n",
    "                df = pdr.get_data_stooq(symbols=ticker, \n",
    "                                        start=self.fecha_inicio, \n",
    "                                        end=self.fecha_final)\n",
    "            elif source == \"yahoo\":\n",
    "                # yf.download también acepta objetos datetime\n",
    "                df = yf.download(ticker, \n",
    "                                 start=self.fecha_inicio, \n",
    "                                 end=self.fecha_final,\n",
    "                                 progress=False)\n",
    "            else:\n",
    "                logger.error(f\"Fuente {source} no soportada\")\n",
    "                return None\n",
    "            \n",
    "            # Ordenar el DataFrame cronológicamente usando el índice (fechas)\n",
    "            df = df.sort_index(ascending=True)\n",
    "            \n",
    "            # Guardar en caché para evitar descargas futuras innecesarias\n",
    "            self._save_to_cache(ticker, source, df)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as error:\n",
    "            logger.error(f\"Error al obtener datos de {source} para {ticker}: {error}\")\n",
    "            return None\n",
    "\n",
    "    def _process_ticker(self, args: Tuple[str, str]) -> Optional[Tuple[str, pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Función auxiliar para el procesamiento en paralelo.\n",
    "        \n",
    "        Args:\n",
    "            args: Una tupla que contiene el ticker y la fuente.\n",
    "            \n",
    "        Returns:\n",
    "            Una tupla (clave, DataFrame) si la descarga es exitosa, o None en caso de error.\n",
    "        \"\"\"\n",
    "        ticker, source = args\n",
    "        df = self._get_data_from_source(ticker, source)\n",
    "        if df is not None:\n",
    "            return (f\"{ticker}_{source}\", df)\n",
    "        return None\n",
    "\n",
    "    def load_parallel(self, tickers: List[str], \n",
    "                      sources: List[str] = [\"stooq\", \"yahoo\"],\n",
    "                      executor_type: str = \"thread\") -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carga datos en paralelo usando un pool de hilos o procesos.\n",
    "        \n",
    "        Debido a que la descarga de datos es una operación IO-bound, se recomienda por defecto\n",
    "        usar hilos (ThreadPoolExecutor). Se puede cambiar a procesos pasando executor_type=\"process\".\n",
    "        \n",
    "        Args:\n",
    "            tickers: Lista de símbolos (tickers) a descargar.\n",
    "            sources: Lista de fuentes desde las cuales se descargan los datos.\n",
    "            executor_type: \"thread\" para ThreadPoolExecutor o \"process\" para ProcessPoolExecutor.\n",
    "            \n",
    "        Returns:\n",
    "            Un diccionario donde la clave es el ticker concatenado con la fuente y el valor es el DataFrame.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        # Se genera la lista de tareas para cada combinación de ticker y fuente\n",
    "        tasks = [(ticker, source) for ticker in tickers for source in sources]\n",
    "        \n",
    "        # Determinar el número óptimo de trabajadores basado en el número de CPUs o tareas\n",
    "        num_cpus = multiprocessing.cpu_count()\n",
    "        num_workers = min(num_cpus, len(tasks))\n",
    "        \n",
    "        # Seleccionar el tipo de ejecutor según el argumento\n",
    "        if executor_type.lower() == \"process\":\n",
    "            Executor = ProcessPoolExecutor\n",
    "            logger.info(\"Usando ProcessPoolExecutor para procesamiento en paralelo\")\n",
    "        else:\n",
    "            Executor = ThreadPoolExecutor\n",
    "            logger.info(\"Usando ThreadPoolExecutor para procesamiento en paralelo\")\n",
    "        \n",
    "        # Ejecutar las tareas en paralelo\n",
    "        with Executor(max_workers=num_workers) as executor:\n",
    "            for result in executor.map(self._process_ticker, tasks):\n",
    "                if result:\n",
    "                    key, df = result\n",
    "                    results[key] = df\n",
    "                    \n",
    "        return results\n",
    "\n",
    "    def plot_prices(self, data: Dict[str, pd.DataFrame], \n",
    "                    column: str = \"Close\") -> None:\n",
    "        \"\"\"\n",
    "        Grafica los precios de cierre de múltiples activos.\n",
    "        \n",
    "        Args:\n",
    "            data: Diccionario de DataFrames con datos financieros.\n",
    "            column: Columna que se desea graficar (por defecto \"Close\").\n",
    "        \"\"\"\n",
    "        # Se establece un estilo moderno para el gráfico\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize=(22, 12))\n",
    "        \n",
    "        # Graficar la columna indicada para cada activo\n",
    "        for key, df in data.items():\n",
    "            if column in df.columns:\n",
    "                df[column].plot(label=key, linewidth=2, alpha=0.8)\n",
    "            else:\n",
    "                logger.warning(f\"La columna '{column}' no se encontró en los datos de {key}\")\n",
    "                \n",
    "        plt.title(\"Precios de Cierre\", size=25, pad=20)\n",
    "        plt.xlabel(\"Fecha\", size=20)\n",
    "        plt.ylabel(\"Precios\", size=20)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c49fc-ae45-49b6-a176-d3c9f854a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook():\n",
    "    \"\"\"\n",
    "    Configura el entorno del notebook para mostrar gráficos inline.\n",
    "    \n",
    "    Esto es útil cuando se ejecuta en Jupyter.\n",
    "    \"\"\"\n",
    "    import IPython\n",
    "    IPython.get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    plt.style.use('seaborn')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855c42a-6ff8-408f-be98-365e4fcd466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Ejemplo de uso del DataLoader.\n",
    "    \n",
    "    Se configura el entorno (en caso de estar en un notebook), se descargan datos de varios\n",
    "    tickers en paralelo y se grafican los precios de cierre.\n",
    "    \"\"\"\n",
    "    # Configurar el entorno del notebook\n",
    "    try:\n",
    "        setup_notebook()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"No se pudo configurar el notebook: {e}\")\n",
    "    \n",
    "    # Inicializar el cargador de datos\n",
    "    loader = DataLoader()\n",
    "    \n",
    "    # Definir una lista de tickers a procesar\n",
    "    tickers = [\"AMZN\", \"AAPL\", \"MSFT\", \"GOOGL\", \"META\"]\n",
    "    \n",
    "    # Cargar datos en paralelo utilizando ThreadPoolExecutor (por defecto IO-bound)\n",
    "    logger.info(\"Iniciando carga de datos...\")\n",
    "    data = loader.load_parallel(tickers, executor_type=\"thread\")\n",
    "    logger.info(f\"Datos cargados exitosamente para {len(data)} combinaciones de ticker y fuente\")\n",
    "    \n",
    "    # Graficar los precios de cierre\n",
    "    loader.plot_prices(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FinancialDataLoader] *",
   "language": "python",
   "name": "conda-env-FinancialDataLoader-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
